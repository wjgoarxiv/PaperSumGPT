{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjgoarxiv/PaperSumGPT/blob/main/%5B%EC%B1%97GPT_MS%EC%95%A0%EC%A0%80%ED%86%A4%5D_Team_%EB%91%90%EB%B6%80_%EB%85%BC%EB%AC%B8%EC%9A%94%EC%95%BD%EA%B8%B0_V1_0_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y86AHK4ICDG_"
      },
      "source": [
        "# **ë…¼ë¬¸ ìš”ì•½ê¸° V1.0.0**\n",
        "> Written by __Team ë‘ë¶€__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "J48xwJH3T2Aa"
      },
      "outputs": [],
      "source": [
        "#@title ## **1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜í•˜ê¸°**\n",
        "#@markdown ì™¼ìª½ì˜ __\"ì‹¤í–‰\"__ ë²„íŠ¼ì„ ëˆŒëŸ¬ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì„¤ì¹˜í•´ì£¼ì„¸ìš”. <br/> ì„¤ì¹˜ì—ëŠ” ë‹¤ì†Œ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆì–´ìš”. <br/>\n",
        "#@markdown > **ì„¤ì¹˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª¨ìŒ**\n",
        "#@markdown > <br/> - `tqdm` : Progress bar for loops and tasks.\n",
        "#@markdown > <br/> - `pyfiglet` : `ASCII` art text generator.\n",
        "#@markdown > <br/> - `pexpect` : Library for controlling and automating other programs.\n",
        "#@markdown > <br/> - `openai` : To use `ChatGPT` API \n",
        "#@markdown > <br/> - `tabulate` : Pretty-print tabular data.\n",
        "#@markdown > <br/> - `fitz` : Python binding for `MuPDF`.\n",
        "#@markdown > <br/> - `pdf2image` : A python module that wraps `pdftoppm` and `pdftocairo` to convert PDF to a PIL Image object.\n",
        "#@markdown > <br/> - `pytesseract` : Wrapper for Google's `Tesseract-OCR` Engine.\n",
        "#@markdown > <br/> - `frontend` : Visualizations for Jupyter notebooks.\n",
        "#@markdown > <br/> - `opencv-contrib-python` : OpenCV's extended modules for computer vision.\n",
        "#@markdown > <br/> - `pillow` : Python Imaging Library (Fork) for opening, manipulating, and saving many different image file formats.\n",
        "#@markdown > <br/> - `chatgpt-wrapper` : An useful open-source unofficial Power CLI, Python API and Flask API that lets us interact programmatically with ChatGPT/GPT4.\n",
        "\n",
        "!pip install tqdm -qq\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "\n",
        "packages = ['pyfiglet', 'pexpect', 'openai', 'tabulate', 'fitz', 'pdf2image', 'pytesseract', 'frontend', 'opencv-contrib-python', 'pillow']\n",
        "for package in tqdm_notebook(packages, desc=\"Installing packages\"):\n",
        "    !pip install $package -qq &> /dev/null\n",
        "\n",
        "!pip install git+https://github.com/mmabrouk/chatgpt-wrapper -qq &> /dev/null # Install `chatgpt-wrapper`\n",
        "!apt-get install poppler-utils -qq &> /dev/null # To progress PDF OCR procedures in Colab\n",
        "!sudo apt-get install tesseract-ocr -qq &> /dev/null # Configure `tesseract-ocr` in Colab\n",
        "import os\n",
        "os.environ['PATH'] += ':/usr/local/bin'\n",
        "!echo \"INFO: ì„¤ì¹˜ê°€ ì™„ë£Œë˜ì—ˆì–´ìš”!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPKuCDWHEtwE"
      },
      "outputs": [],
      "source": [
        "#@title ## **2. API Key ì…ë ¥í•˜ê¸°** \n",
        "#@markdown https://platform.openai.com/account/api-keys ì— ì ‘ì†í•˜ì—¬, API key ê°’ì„ ë³µì‚¬í•´ì£¼ì„¸ìš”. <br/> __\"ì‹¤í–‰\"__ ë²„íŠ¼ì„ í´ë¦­í•œ í›„, ë³µì‚¬í•œ API key ê°’ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. \n",
        "\n",
        "import os \n",
        "import openai \n",
        "\n",
        "your_api = input(\"INFO: ë³µì‚¬í•œ API keyë¥¼ ë¶™ì—¬ë„£ì–´ì£¼ì„¸ìš”: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = your_api\n",
        "os.system('cls' if os.name == 'nt' else 'clear')\n",
        "print(\"INFO: API keyê°€ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆì–´ìš”!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvgvD6eAYg0L"
      },
      "outputs": [],
      "source": [
        "#@title ##**3. ChatGPT API ì‹¤í–‰ ì¤€ë¹„í•˜ê¸°** \n",
        "#@markdown __\"ì‹¤í–‰\"__ ë²„íŠ¼ì„ ì…ë ¥í•˜ì—¬ ChatGPT APIë¥¼ ì‹¤í–‰í•  ì¤€ë¹„ë¥¼ ì™„ë£Œí•˜ì„¸ìš”.\n",
        "\n",
        "!apt-get install sqlite3 &> /dev/null\n",
        "\n",
        "import pexpect\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set the path to the SQLite binary\n",
        "sqlite_path = \"/lib/x86_64-linux-gnu/libsqlite3.so.0\"\n",
        "\n",
        "# Set the path to the SQLite database file\n",
        "db_path = \"////root/.local/share/chatgpt-wrapper/profiles/default/storage.db\"\n",
        "\n",
        "# Define the SQLite command to execute\n",
        "sqlite_command = \"{} {}\"\n",
        "\n",
        "# Start the SQLite process and connect to the database\n",
        "process = pexpect.spawn(\"chatgpt\", encoding=\"utf-8\")\n",
        "process.expect(\"Enter username\")\n",
        "\n",
        "# Send the username to the SQLite process and expect a prompt\n",
        "process.sendline(\"UserID1\")\n",
        "process.expect(\"Enter email\")\n",
        "\n",
        "# Send a blank password to the SQLite process and expect a prompt\n",
        "process.sendline(\"\")\n",
        "process.expect(\"Enter password\")\n",
        "\n",
        "# Send a blank email to the SQLite process and expect a prompt\n",
        "process.sendline(\"\")\n",
        "process.expect(\"Provide a prompt\")\n",
        "print(process.before)\n",
        "\n",
        "# Send a command to log in using pexpect\n",
        "process.sendline(\"/login UserID1\")\n",
        "process.expect(\"userid1@\")\n",
        "\n",
        "# Send a command to exit the SQLite process\n",
        "process.sendline(\"/exit\")\n",
        "process.wait()\n",
        "\n",
        "tqdm.write(\"INFO: ChatGPT APIë¥¼ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆì–´ìš”.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBlOYRYPmbw6"
      },
      "outputs": [],
      "source": [
        "#@title ## **4. ì¸í’‹ íŒŒì¼ Colabì— ì—…ë¡œë“œí•˜ê¸°**\n",
        "#@markdown __\"ì‹¤í–‰\"__ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë…¼ë¬¸ ìš”ì•½ì„ ì§„í–‰ í•  íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”. <br/>\n",
        "#@markdown ì¸í’‹ íŒŒì¼ë¡œëŠ” `.md`, `.txt`, ê·¸ë¦¬ê³  `.pdf` ì„¸ ê°€ì§€ í˜•íƒœì˜ íŒŒì¼ íƒ€ì…ì„ ì§€ì›í•˜ê³  ìˆìŠµë‹ˆë‹¤. <br/>\n",
        "#@markdown > __âš ï¸ NOTE__: PDF íŒŒì¼ì„ ì„ íƒí•˜ë©´, OCR ìŠ¤ìºë‹ì„ ì´ìš©í•´ ì´ë¯¸ì§€ë¡œë¶€í„° í…ìŠ¤íŠ¸ë“¤ì„ ë¶ˆëŸ¬ì˜¤ê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì§ì ‘ ì •ë¦¬í•˜ì—¬ ë†“ì€ í…ìŠ¤íŠ¸ íŒŒì¼ë³´ë‹¤ ì •í™•ë„ê°€ ë‹¤ì†Œ ë–¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ì , ë¯¸ë¦¬ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤.\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "print(\"INFO: íŒŒì¼(ë“¤)ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆì–´ìš”!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kTF0fnC4YUlI"
      },
      "outputs": [],
      "source": [
        "#@title ## **5. ë…¼ë¬¸ ìš”ì•½ê¸° ì‹¤í–‰í•˜ê¸°**\n",
        "#@markdown __\"ì‹¤í–‰\"__ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë…¼ë¬¸ ìš”ì•½ê¸°ë¥¼ ì‹¤í–‰í•´ë³´ì„¸ìš”.\n",
        "\n",
        "# ------------------ Import libraries ------------------ #\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import pyfiglet\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "from chatgpt_wrapper import OpenAIAPI\n",
        "from google.colab import files\n",
        "\n",
        "# Manipulating PDF\n",
        "import cv2\n",
        "import pytesseract as tess\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------ Main code starts ------------------ #\n",
        "# Print the title \n",
        "os.system('cls' if os.name == 'nt' else 'clear')\n",
        "title = pyfiglet.figlet_format('PaperSumGPT', font = 'small')\n",
        "print('\\n')\n",
        "print(title+'\\n')\n",
        "print('\\n')\n",
        "print('------------------------------------------------')\n",
        "print('ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì œ ì´ë©”ì¼ë¡œ ë©”ì¼ì„ ë³´ë‚´ì£¼ì„¸ìš”.')\n",
        "print('\\nì—ëŸ¬ë‚˜ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ì•Œë ¤ì£¼ì„¸ìš”.')\n",
        "print('\\n ğŸ“¨ woo_go@yahoo.com')\n",
        "print('\\nì œ ê¹ƒí—ˆë¸Œ (https://github.com/wjgoarxiv/papersumgpt)ë¥¼ ë°©ë¬¸í•˜ì…”ì„œ ë” ë§ì€ ì •ë³´ë¥¼ ì–»ì–´ë³´ì„¸ìš”.')\n",
        "print('------------------------------------------------')\n",
        "\n",
        "def main():\n",
        "    # Ask user if the brought input file is a markdown file or plain text file \n",
        "    print('\\n')\n",
        "    file_type = int(input(\"\"\"INFO: ì¸í’‹ íŒŒì¼ë¡œ ë„£ê³ ì í•˜ëŠ” íŒŒì¼ì˜ í™•ì¥ìë¥¼ ë²ˆí˜¸ (1~3)ë¡œ ì„ íƒí•´ì£¼ì„¸ìš”.\n",
        "\n",
        "    1. Markdown (`.md`) file\n",
        "    2. Plain text (`.txt`) file\n",
        "    3. PDF (`.pdf`) file\n",
        "\n",
        "    : \"\"\"))\n",
        "    print('\\n')\n",
        "    print('------------------------------------------------')\n",
        "\n",
        "    # Print the list of files in the current directory\n",
        "    if file_type == 1:\n",
        "        file_list = glob.glob('./*.md')\n",
        "        file_list = [file.split('\\\\')[-1] for file in file_list]\n",
        "        file_list.sort()\n",
        "\n",
        "    elif file_type == 2:\n",
        "        file_list = glob.glob('./*.txt')\n",
        "        file_list = [file.split('\\\\')[-1] for file in file_list]\n",
        "        file_list.sort()\n",
        "\n",
        "    elif file_type == 3:\n",
        "        file_list = glob.glob('./*.pdf')\n",
        "        file_list = [file.split('\\\\')[-1] for file in file_list]\n",
        "        file_list.sort()\n",
        "\n",
        "    # File not found error handling\n",
        "    try: \n",
        "        if len(file_list) == 0:\n",
        "            raise FileNotFoundError\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    # Alert the user \n",
        "    except: \n",
        "        print('ERROR: í˜„ì¬ ë””ë ‰í„°ë¦¬ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.')\n",
        "        print('------------------------------------------------')\n",
        "        sys.exit()\n",
        "\n",
        "    # Print the list of files in the current directory\n",
        "    file_num = [] \n",
        "    for i in range(len(file_list)):\n",
        "        file_num.append(i+1)\n",
        "    print(tabulate({'File number': file_num, 'File name': file_list}, headers='keys', tablefmt='psql'))\n",
        "    print('------------------------------------------------')\n",
        "    user_input = int(input('\\nINFO: íŒŒì¼ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ê±°ë‚˜ \"0\"ì„ ëˆŒëŸ¬ ì¢…ë£Œí•˜ì„¸ìš”: '))\n",
        "\n",
        "    if user_input == 0:\n",
        "        print(\"INFO: í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "        sys.exit()\n",
        "    else:\n",
        "        try: \n",
        "            print(\"INFO: ì‚¬ìš©í•  íŒŒì¼ ì´ë¦„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: \", file_list[user_input-1])\n",
        "        except:\n",
        "            print(\"ERROR: ì…ë ¥ ë²ˆí˜¸ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. íŒŒì¼ ë²ˆí˜¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "            print(\"ERROR: í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "            sys.exit()\n",
        "\n",
        "    # Ask user to turn on `verbose` mode. \n",
        "    # If the user turns on `verbose` mode, the program will print the intermediate results.\n",
        "    print('------------------------------------------------')\n",
        "    verbose = input(\"INFO: verbose mode (ìƒì„¸ ëª¨ë“œ)ë¥¼ ì¼œì‹œê² ì–´ìš”? ìƒì„¸ ëª¨ë“œë¥¼ ì¼œë©´ ì¤‘ê°„ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³¼ ìˆ˜ ìˆì–´ìš”. (y/n): \")\n",
        "    if verbose == 'y' or verbose == 'Y':\n",
        "        verbose = True\n",
        "    elif verbose == 'n' or verbose == 'N':\n",
        "        verbose = False\n",
        "    else:\n",
        "        print(\"ERROR: ì˜ëª»ëœ ì‘ë‹µì„ ì…ë ¥í•˜ì…¨ì–´ìš”. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤...\")\n",
        "        sys.exit()\n",
        "    print('------------------------------------------------')\n",
        "\n",
        "    # Ask user their desired ChatGPT model\n",
        "    chatgpt_model = input(\"\"\"INFO: ì‚¬ìš©í•  ChatGPT ëª¨ë¸ì˜ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”:\n",
        "\n",
        "    1. default (Turbo version for ChatGPT Plus users and default version for free users)\n",
        "    2. gpt4 (Only available for ChatGPT Plus users; a little bit slower than the default model)\n",
        "    3. legacy (Only available for ChatGPT Plus users; an older version of the default model)\n",
        "\n",
        "    \"\"\")\n",
        "                        \n",
        "    if chatgpt_model == '1':\n",
        "        chatgpt_model = 'default'\n",
        "    elif chatgpt_model == '2':\n",
        "        chatgpt_model = 'gpt4'\n",
        "    elif chatgpt_model == '3':\n",
        "        chatgpt_model = 'legacy'\n",
        "    else:\n",
        "        print(\"ERROR: ì…ë ¥ ë²ˆí˜¸ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ì–´ìš”. íŒŒì¼ ë²ˆí˜¸ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.\")\n",
        "        print(\"ERROR: í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. \")\n",
        "        sys.exit()\n",
        "    print('------------------------------------------------')\n",
        "\n",
        "    # ------------------ Convert pdf to markdown ------------------ #\n",
        "    # This process requires following processes: \n",
        "    # 1. Convert pdf to images\n",
        "    # 2. Perform OCR\n",
        "    # 3. Convert the images to a markdown file \n",
        "\n",
        "    def pdf_to_images(pdf_file):\n",
        "        return convert_from_path(pdf_file, 500)\n",
        "    \n",
        "    def process_image(image):\n",
        "        # Convert PIL image to OpenCV image\n",
        "        original_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "        gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
        "        _, threshold_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "        rectangular_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (11, 11))\n",
        "        dilated_image = cv2.dilate(threshold_image, rectangular_kernel, iterations=1)\n",
        "\n",
        "        contours, hierarchy = cv2.findContours(dilated_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        copied_image = original_image.copy()\n",
        "\n",
        "        ocr_text = \"\"\n",
        "\n",
        "        for cnt in contours:\n",
        "            x, y, w, h = cv2.boundingRect(cnt)\n",
        "            cropped = copied_image[y:y + h, x:x + w]\n",
        "            text = tess.image_to_string(cropped, config='--oem 3 --psm 1')\n",
        "            ocr_text += text\n",
        "\n",
        "        return ocr_text\n",
        "    \n",
        "    def perform_ocr(images):\n",
        "        ocr_text = \"\"\n",
        "        for i, image in tqdm(enumerate(images), total=len(images)):\n",
        "            text = process_image(image)\n",
        "            ocr_text += f\"Page {i+1}:\\n{text}\\n\\n\"\n",
        "        return ocr_text\n",
        "    \n",
        "    if file_type == 3:\n",
        "        print('INFO: PDF íŒŒì¼ì„ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ë³€í™˜í•˜ëŠ” ì¤‘...')\n",
        "        pdf_text = perform_ocr(pdf_to_images(file_list[user_input-1]))\n",
        "\n",
        "        with open(file_list[user_input-1] + '_markdowned.md', 'w', encoding='utf-8') as f:\n",
        "            for line in pdf_text:\n",
        "                f.write(line)\n",
        "\n",
        "        file_list[user_input-1] = file_list[user_input-1] + '_markdowned.md'\n",
        "\n",
        "        print('INFO: PDF íŒŒì¼ì´ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.')\n",
        "        print('------------------------------------------------')\n",
        "\n",
        "    # ------------------ Function starts ------------------ #\n",
        "\n",
        "    # Initialize ChatGPT\n",
        "    bot = OpenAIAPI()\n",
        "\n",
        "    # read input file\n",
        "    with open(file_list[user_input-1], 'r') as f:\n",
        "        input_text = f.read()\n",
        "\n",
        "    # Ask user maximum length of input text (if don't know, just input 3000)\n",
        "    max_length = 7000\n",
        "\n",
        "    # truncate input text into smaller parts\n",
        "    input_parts = [input_text[i:i+max_length] for i in range(0, len(input_text), max_length)]\n",
        "\n",
        "    # define initial prompt message\n",
        "    initial_prompt = \"Please, act as 'High-quality content abbreviator'. Since you have the input limits (OpenAI limited your input limit), you have to firstly take the all the inputs iteratively. To do this, I've already truncated the long inputs into each subpart. You'll now have to take the inputs iteratively. The important thing is that you should NOT answer directly or respond to the previous message. Make sure that you have to accomplish the task when all the inputs are given. I'll let you know if all the inputs are given.\"\n",
        "\n",
        "    # send initial prompt message to ChatGPT\n",
        "    print(\"INFO: Tossing initial prompt...\")\n",
        "    success, response, message = bot.ask_stream(initial_prompt) # ask_stream is required; since we have to make ChatGPT API understands the context.\n",
        "    for chunk in bot.ask_stream(initial_prompt):\n",
        "        print(chunk)\n",
        "\n",
        "    # initialize response list\n",
        "    response_parts = []\n",
        "\n",
        "    # define prompt message while iterating over input parts and send them to ChatGPT\n",
        "    for i, part in enumerate(input_parts):\n",
        "        if i == len(input_parts) - 1:\n",
        "            prompt = f\"This is the {i+1}th part of the truncated input contents. And PLEASE! Do NOT answer and if you understood the input, just keep asking me to input the leftover contents.\\n\\n```\\n{part}\\n```\\nThank you for providing all the inputs.\"\n",
        "        else:\n",
        "            prompt = f\"This is the {i+1}th part of the truncated input contents. And PLEASE! Do NOT answer and if you understood the input, just keep asking me to input the leftover contents.\\n\\n```\\n{part}\\n```\"\n",
        "\n",
        "        print(f\"INFO: ì§„í–‰ ì¤‘... ({i+1}/{len(input_parts)})\")\n",
        "\n",
        "    # send prompt message and prompt part to ChatGPT\n",
        "        for chunk in bot.ask_stream(prompt):\n",
        "            response_parts.append(chunk)\n",
        "\n",
        "    # Handling the `verbose` mode \n",
        "        if verbose == True:\n",
        "            print(f\"INFO: {i+1}/{len(input_parts)} ë¶€ë¶„ì˜ ì „ë‹¬ëœ ë‚´ìš©: {prompt}\")\n",
        "            print(f\"INFO: ChatGPTì˜ ì‘ë‹µ: {response}\")\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "\n",
        "    # define final prompt message\n",
        "    final_prompt = \"\"\"Now, all the inputs are given to you. You should co   mbine and abbreviate all the inputs by fitting into the following markdown format. The markdown format is as follows:\n",
        "    \n",
        "    ------ TEMPLATE STARTS ------\n",
        "\n",
        "    # **[TITLE]**\n",
        "    (Bring the title from the foremost heading in the document. The powerful hint is that the title comes before the people who wrote the document.)\n",
        "\n",
        "    ## **Introduction**\n",
        "\n",
        "    ## **Methodology**\n",
        "    ### **Apparatus**\n",
        "    ### **Experimental procedure**\n",
        "    ### **Computational procedure (if exists)**\n",
        "    ### **Data analysis**\n",
        "\n",
        "    ## **Results & discussion**\n",
        "\n",
        "    ## **Conclusions**\n",
        "\n",
        "    ## **Significance of this study**\n",
        "\n",
        "    ## **Things to look out for in follow-up research**\n",
        "\n",
        "    ### **Useful references to consider**\n",
        "    ...\n",
        "\n",
        "    ------ TEMPLATE ENDS ------\n",
        "\n",
        "    And please, write the outputs thinking you are writing PPT slides. But NOT too simple. You have to write the outputs in a way that the readers can understand the contents easily.\n",
        "    Consecutively, if possible, please find some useful references (including title and authors) from Text or Markdown input file, and re-write them into `### Useful references to consider` subheader. \n",
        "    \"\"\"\n",
        "\n",
        "    # send final prompt message to ChatGPT\n",
        "    print(\"INFO: ë§ˆì§€ë§‰ í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤...\")\n",
        "\n",
        "    # join response parts to form final response\n",
        "    # If final response doesn't exist, use the last response part\n",
        "    try: \n",
        "        final_response = response_parts[-1]\n",
        "    except:\n",
        "        print(\"ERROR: ëª©ë¡ ì¸ë±ìŠ¤ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. ì¢…ë£Œ ì¤‘...\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    success, response, message = bot.ask(final_prompt)\n",
        "    if success:\n",
        "        final_response = response  # Change this line\n",
        "        print(f\"INFO: ChatGPTì˜ ì‘ë‹µ: {response}\")\n",
        "    else:\n",
        "        raise RuntimeError(message)\n",
        "    \n",
        "    count_yes = 0\n",
        "    again_final_prompt_base = \"I think you are not done yet. Please input the leftover contents.\"\n",
        "\n",
        "    # Create a variable to store the concatenated responses\n",
        "    concatenated_responses = \"\"\n",
        "\n",
        "    while True: \n",
        "        user_input = input(\"\\nINFO: ë‹µë³€ì´ ì˜ë¦° ê²ƒ ê°™ë‚˜ìš”? (y/n): \")\n",
        "        if user_input.strip().lower() == \"y\":\n",
        "            count_yes += 1 \n",
        "            print(\"\\nINFO: ì¶”ê°€ ë‹µë³€ì„ ìš”ì²­í•©ë‹ˆë‹¤...\")\n",
        "            last_response_part = response.strip().split()[-1] # Get the last word\n",
        "            again_final_prompt = f\"{again_final_prompt_base}\" + \"\\n\" + \"However, keep in mind that you SHOULD NOT PRINT THE TEMPLATE that I gave you now on; JUST KEEP GENERATING from the truncated part. NEVER RESTART the conversation. Thank you.\"\n",
        "            success, response, message = bot.ask(again_final_prompt)\n",
        "            if success: \n",
        "                # Find the overlapping part between the last response and the new response\n",
        "                overlap_start = response.find(last_response_part)\n",
        "                if overlap_start != -1:\n",
        "                    response = response[overlap_start + len(last_response_part):]\n",
        "\n",
        "                concatenated_responses += response  # Append the response without the overlapping part to the concatenated_responses\n",
        "                print(f\"INFO: ì´ì–´ì§„ ChatGPTì˜ ì‘ë‹µ: {concatenated_responses}\")\n",
        "            else:\n",
        "                raise RuntimeError(message)\n",
        "        elif user_input.strip().lower() == \"n\":\n",
        "            break\n",
        "        else: \n",
        "            print(\"ERROR: ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\")\n",
        "\n",
        "    # Concatenate all the response parts upto the number of times the user says yes. Direction: end to start\n",
        "    final_response = final_response + \"\\n\" + concatenated_responses\n",
        "\n",
        "    # prompt user to choose output format\n",
        "    output_format = input(\"\\nINFO ì›í•˜ì‹œëŠ” ì¶œë ¥ í˜•ì‹ (stream / txt / md)ì„ ì„ íƒí•˜ì„¸ìš”: \")\n",
        "\n",
        "    # define maximum length of each response part to be printed at once\n",
        "    max_response_length = 3000\n",
        "\n",
        "    if output_format.lower() == \"stream\":\n",
        "        # print response parts until the full response is generated\n",
        "        i = 0\n",
        "        while i < len(final_response):\n",
        "            # print next response part\n",
        "            response_part = final_response[i:i+max_response_length]\n",
        "            print(response_part)\n",
        "            i += len(response_part)\n",
        "\n",
        "            # if there are more response parts, ask the user to continue\n",
        "            if i < len(final_response):\n",
        "                user_input = input(\"INFO: Enter í‚¤ë¥¼ ëˆŒëŸ¬ ê³„ì† ì§„í–‰í•˜ê±°ë‚˜ 'quit'ì„ ì…ë ¥í•˜ì—¬ ì¢…ë£Œí•©ë‹ˆë‹¤:\")\n",
        "                if user_input.strip().lower() == \"quit\":\n",
        "                    break\n",
        "                else: \n",
        "                    print(\"ERROR: ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ì¢…ë£Œ ì¤‘...\")\n",
        "\n",
        "    # Export the file name as same as the input file name (`file_list[user_input-1]`)\n",
        "    elif output_format.lower() == \"txt\":\n",
        "        # write response to a text file\n",
        "        with open(\"OUTPUT.txt\", \"w\") as f:\n",
        "            f.write(final_response)\n",
        "        print(\"INFO: OUTPUT.txt íŒŒì¼ì´ ì €ì¥ë˜ì—ˆì–´ìš”.\")\n",
        "        files.download('./OUTPUT.txt')\n",
        "    elif output_format.lower() == \"md\":\n",
        "        # write response to a markdown file\n",
        "        with open(\"OUTPUT.md\", \"w\") as f:\n",
        "            f.write(f\"\\n{final_response}\\n\")\n",
        "        print(\"INFO: Output.md íŒŒì¼ì´ ì €ì¥ë˜ì—ˆì–´ìš”.\")\n",
        "        files.download('./OUTPUT.md')\n",
        "    else:\n",
        "        print(\"ERROR: ì˜ëª»ëœ ì¶œë ¥ í˜•ì‹ì„ ì„ íƒí•˜ì…¨ì–´ìš”. 'stream', 'txt' ë˜ëŠ” 'md'ë¥¼ ì„ íƒí•˜ì„¸ìš”.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-DE5n1sxnXLp"
      },
      "outputs": [],
      "source": [
        "#@title ## **âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì˜€ë‹¤ë©´?**\n",
        "#@markdown __\"ì‹¤í–‰\"__ ë²„íŠ¼ì„ ëˆŒëŸ¬ ëŸ°íƒ€ì„ì„ ì´ˆê¸°í™”í•˜ì„¸ìš”. <br/> \n",
        "#@markdown ê·¸ í›„, `1ë²ˆ`ë¶€í„° ë‹¤ì‹œ ì§„í–‰í•´ë³´ì„¸ìš”. \n",
        "\n",
        "print('INFO: ì½”ë© ëŸ°íƒ€ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. 1ë²ˆë¶€í„° ë‹¤ì‹œ ì§„í–‰í•´ë³´ì„¸ìš”.')\n",
        "import time \n",
        "time.sleep(1)\n",
        "!kill -9 -1\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjXPzyDe51aY"
      },
      "source": [
        "# ë ˆê±°ì‹œ ë²„ì ¼ í…ŒìŠ¤íŒ…\n",
        "- Use ChatGPT instead of OpenAIAPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xITR8O8f54Sz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "5ec726c2e02e4369870dfff4714b5365",
            "41fb03fcd73140d48d09beaf1946e4a4",
            "e061d6ec87eb4e33b4b442c5897699f3",
            "a26e5b4f9bc54dd4b550717a49897611",
            "3fb39533df084d579f66ed8999e74c1c",
            "355ecc3fdda94410a029fcb4e3126ef3",
            "8ed3f71369714433bbff293fa087b048",
            "f825d7c1fe5c474db9424d2b1d46f107",
            "a80b95c676994791aacdf8aa389b35af",
            "6bd02751fd7e4acaa999c5e0c4cff290",
            "5538331860944af6aaa266ff94dd2bea"
          ]
        },
        "outputId": "d1a7b21a-3c20-4c92-f028-3196ba02dea5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Installing packages:   0%|          | 0/11 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ec726c2e02e4369870dfff4714b5365"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for PaperSumGPT (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: ì„¤ì¹˜ê°€ ì™„ë£Œë˜ì—ˆì–´ìš”!\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm -qq\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "\n",
        "packages = ['pyfiglet', 'pexpect', 'openai', 'tabulate', 'fitz', 'pdf2image', 'pytesseract', 'frontend', 'opencv-contrib-python', 'pillow', 'playwright']\n",
        "for package in tqdm_notebook(packages, desc=\"Installing packages\"):\n",
        "    !pip install $package -qq &> /dev/null\n",
        "\n",
        "!git clone https://github.com/wjgoarxiv/papersumgpt.git &> /dev/null\n",
        "!chmod +x ./papersumgpt/install_old-repo.sh; ./papersumgpt/install_old-repo.sh &> /dev/null\n",
        "!pip install ./papersumgpt -qq\n",
        "\n",
        "!apt-get install poppler-utils -qq &> /dev/null # To progress PDF OCR procedures in Colab\n",
        "!sudo apt-get install tesseract-ocr -qq &> /dev/null # Configure `tesseract-ocr` in Colab\n",
        "import os\n",
        "os.environ['PATH'] += ':/usr/local/bin'\n",
        "!echo \"INFO: ì„¤ì¹˜ê°€ ì™„ë£Œë˜ì—ˆì–´ìš”!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW2B-D6U6-CW"
      },
      "outputs": [],
      "source": [
        "!playwright install &> /dev/null\n",
        "!apt-get install libxtst6 libdbus-glib-1-2 &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn9zYeOM8hcH"
      },
      "outputs": [],
      "source": [
        "!apt-get update &> /dev/null\n",
        "!apt-get install -y xvfb &> /dev/null\n",
        "!pip install pyvirtualdisplay -qq\n",
        "from pyvirtualdisplay import Display \n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "!apt-get install sqlite3 &> /dev/null\n",
        "\n",
        "import pexpect\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set the path to the SQLite binary\n",
        "sqlite_path = \"/lib/x86_64-linux-gnu/libsqlite3.so.0\"\n",
        "\n",
        "# Set the path to the SQLite database file\n",
        "db_path = \"////root/.local/share/chatgpt-wrapper/profiles/default/storage.db\"\n",
        "\n",
        "# Define the SQLite command to execute\n",
        "sqlite_command = \"{} {}\"\n",
        "\n",
        "# Start the SQLite process and connect to the database\n",
        "process = pexpect.spawn(\"chatgpt\", encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmWW2zVPBJqw",
        "outputId": "3200f260-a360-4947-9b30-560a2c2b64fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: ë³µì‚¬í•œ API keyë¥¼ ë¶™ì—¬ë„£ì–´ì£¼ì„¸ìš”: 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\n",
            "INFO: API keyê°€ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆì–´ìš”!\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "import openai \n",
        "\n",
        "your_api = input(\"INFO: ë³µì‚¬í•œ API keyë¥¼ ë¶™ì—¬ë„£ì–´ì£¼ì„¸ìš”: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = your_api\n",
        "os.system('cls' if os.name == 'nt' else 'clear')\n",
        "print(\"INFO: API keyê°€ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆì–´ìš”!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        },
        "id": "Kf4zY1yq_ti3",
        "outputId": "03138aab-7935-4e49-838a-99644fed4323"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f0b214b0-08cd-4660-92c6-a55ad9229a9c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f0b214b0-08cd-4660-92c6-a55ad9229a9c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving README.md to README.md\n",
            "INFO: íŒŒì¼(ë“¤)ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆì–´ìš”!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "print(\"INFO: íŒŒì¼(ë“¤)ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆì–´ìš”!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRBJ3DelC2sZ"
      },
      "outputs": [],
      "source": [
        "#@markdown Changing the `backend` setting in the `config.yaml` file.\n",
        "!cp /content/chatgpt-wrapper-5de50d92d2b9937165f22463350b1fac660e8e54/config.sample.yaml /root/.config/chatgpt-wrapper/profiles/default/config.yaml \n",
        "!sed -i 's/backend: chatgpt-browser/backend: chatgpt-api/' /root/.config/chatgpt-wrapper/profiles/default/config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_TcvsxQ6WDu",
        "outputId": "78bc6eb9-58cd-4804-d15f-220cf0bff0bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " (leave blank for passwordless login): \r\n",
            "\u001b[1;32mUser successfully registered.\u001b[0m\r\n",
            "\r\n",
            "\u001b[1;32mLogin successful.\u001b[0m\r\n",
            "\r\n",
            "\r\n",
            "       \u001b[1m\n",
            "INFO: ChatGPT APIë¥¼ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆì–´ìš”.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install sqlite3 &> /dev/null\n",
        "\n",
        "import pexpect\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set the path to the SQLite binary\n",
        "sqlite_path = \"/lib/x86_64-linux-gnu/libsqlite3.so.0\"\n",
        "\n",
        "# Set the path to the SQLite database file\n",
        "db_path = \"////root/.local/share/chatgpt-wrapper/profiles/default/storage.db\"\n",
        "\n",
        "# Define the SQLite command to execute\n",
        "sqlite_command = \"{} {}\"\n",
        "\n",
        "# Start the SQLite process and connect to the database\n",
        "process = pexpect.spawn(\"chatgpt install\", encoding=\"utf-8\")\n",
        "process.expect(\"Enter username\")\n",
        "\n",
        "# Send the username to the SQLite process and expect a prompt\n",
        "process.sendline(\"UserID1\")\n",
        "process.expect(\"Enter email\")\n",
        "\n",
        "# Send a blank password to the SQLite process and expect a prompt\n",
        "process.sendline(\"\")\n",
        "process.expect(\"Enter password\")\n",
        "\n",
        "# Send a blank email to the SQLite process and expect a prompt\n",
        "process.sendline(\"\")\n",
        "process.expect(\"Provide a prompt\")\n",
        "print(process.before)\n",
        "\n",
        "# Send a command to log in using pexpect\n",
        "process.sendline(\"/login UserID1\")\n",
        "process.expect(\"userid1@\")\n",
        "\n",
        "# Send a command to exit the SQLite process\n",
        "process.sendline(\"/exit\")\n",
        "process.wait()\n",
        "\n",
        "tqdm.write(\"INFO: ChatGPT APIë¥¼ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆì–´ìš”.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!papersumgpt"
      ],
      "metadata": {
        "id": "SQp1Luuu03Me",
        "outputId": "a1eb9576-cb51-4cea-c893-f7672c2dfd81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[H\u001b[2J\n",
            "\n",
            " ___                    ___            ___ ___ _____ \n",
            "| _ \\__ _ _ __  ___ _ _/ __|_  _ _ __ / __| _ \\_   _|\n",
            "|  _/ _` | '_ \\/ -_) '_\\__ \\ || | '  \\ (_ |  _/ | |  \n",
            "|_| \\__,_| .__/\\___|_| |___/\\_,_|_|_|_\\___|_|   |_|  \n",
            "         |_|                                         \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------------------------------------------------\n",
            "If you have any questions, please send your questions to my email.\n",
            "\n",
            "Or, please suggest errors and areas that need updating.\n",
            "\n",
            " ğŸ“¨ woo_go@yahoo.com\n",
            "\n",
            "Visit \u001b[;4mhttps://github.com/wjgoarxiv/papersumgpt\u001b[0m for more information.\n",
            "------------------------------------------------\n",
            "\n",
            "\n",
            "\u001b[31;1;mINFO: Please type the number the file type that you want to use:\u001b[0m\n",
            "\n",
            "    1. Markdown (`.md`) file\n",
            "    2. Plain text (`.txt`) file\n",
            "    3. PDF (`.pdf`) file\n",
            "\n",
            "    : 1\n",
            "\n",
            "\n",
            "------------------------------------------------\n",
            "+---------------+-------------+\n",
            "|   File number | File name   |\n",
            "|---------------+-------------|\n",
            "|             1 | ./README.md |\n",
            "+---------------+-------------+\n",
            "------------------------------------------------\n",
            "\n",
            "\u001b[31;1;mINFO: Please select the file number or press \"0\" to exit: \u001b[0m1\n",
            "\u001b[31;1;mINFO: The file name that would be utilized is: \u001b[0m ./README.md\n",
            "------------------------------------------------\n",
            "\u001b[31;1;mINFO: Do you want to turn on `verbose` mode? If you turn on `verbose` mode, the program will print the intermediate results. (y/n): \u001b[0mn\n",
            "------------------------------------------------\n",
            "\u001b[31;1;mINFO: Please type the number the ChatGPT model that you want to use: \u001b[0m\n",
            "\n",
            "    1. default (Turbo version for ChatGPT Plus users and default version for free users)\n",
            "    2. gpt4 (Only available for ChatGPT Plus users; a little bit slower than the default model)\n",
            "    3. legacy (Only available for ChatGPT Plus users; an older version of the default model)\n",
            "\n",
            "    Note that the option 2 and 3 are NOT available for free users. If you are the free user, please select the option 1.\n",
            "    1\n",
            "------------------------------------------------\n",
            "\u001b[31;1;mINFO: Tossing initial prompt...\u001b[0m\n",
            "AsyncChatGPT - ERROR - Failed to decode session key. Maybe Access denied? \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/papersumgpt\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/PaperSumGPT/__main__.py\", line 4, in main\n",
            "    PaperSumGPT.main()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/PaperSumGPT/PaperSumGPT.py\", line 225, in main\n",
            "    success, response, message = bot.ask(initial_prompt)\n",
            "  File \"/content/chatgpt-wrapper-5de50d92d2b9937165f22463350b1fac660e8e54/chatgpt_wrapper/chatgpt.py\", line 570, in ask\n",
            "    return self.async_run(self.agpt.ask(message, title=title))\n",
            "  File \"/content/chatgpt-wrapper-5de50d92d2b9937165f22463350b1fac660e8e54/chatgpt_wrapper/chatgpt.py\", line 546, in async_run\n",
            "    return loop.run_until_complete(awaitable)\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n",
            "    return future.result()\n",
            "  File \"/content/chatgpt-wrapper-5de50d92d2b9937165f22463350b1fac660e8e54/chatgpt_wrapper/chatgpt.py\", line 515, in ask\n",
            "    response = list([i async for i in self.ask_stream(message, title=title)])\n",
            "  File \"/content/chatgpt-wrapper-5de50d92d2b9937165f22463350b1fac660e8e54/chatgpt_wrapper/chatgpt.py\", line 515, in <listcomp>\n",
            "    response = list([i async for i in self.ask_stream(message, title=title)])\n",
            "  File \"/content/chatgpt-wrapper-5de50d92d2b9937165f22463350b1fac660e8e54/chatgpt_wrapper/chatgpt.py\", line 349, in ask_stream\n",
            "    if \"accessToken\" not in self.session:\n",
            "TypeError: argument of type 'NoneType' is not iterable\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSAjPygnrbFuyynj9Xj1Tn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5ec726c2e02e4369870dfff4714b5365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41fb03fcd73140d48d09beaf1946e4a4",
              "IPY_MODEL_e061d6ec87eb4e33b4b442c5897699f3",
              "IPY_MODEL_a26e5b4f9bc54dd4b550717a49897611"
            ],
            "layout": "IPY_MODEL_3fb39533df084d579f66ed8999e74c1c"
          }
        },
        "41fb03fcd73140d48d09beaf1946e4a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_355ecc3fdda94410a029fcb4e3126ef3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8ed3f71369714433bbff293fa087b048",
            "value": "Installing packages: 100%"
          }
        },
        "e061d6ec87eb4e33b4b442c5897699f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f825d7c1fe5c474db9424d2b1d46f107",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a80b95c676994791aacdf8aa389b35af",
            "value": 11
          }
        },
        "a26e5b4f9bc54dd4b550717a49897611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bd02751fd7e4acaa999c5e0c4cff290",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5538331860944af6aaa266ff94dd2bea",
            "value": " 11/11 [01:13&lt;00:00,  6.15s/it]"
          }
        },
        "3fb39533df084d579f66ed8999e74c1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "355ecc3fdda94410a029fcb4e3126ef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ed3f71369714433bbff293fa087b048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f825d7c1fe5c474db9424d2b1d46f107": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a80b95c676994791aacdf8aa389b35af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bd02751fd7e4acaa999c5e0c4cff290": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5538331860944af6aaa266ff94dd2bea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}